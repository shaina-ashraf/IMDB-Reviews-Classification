{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","metadata":{"execution":{"iopub.status.busy":"2022-04-26T17:29:57.374643Z","iopub.execute_input":"2022-04-26T17:29:57.374965Z","iopub.status.idle":"2022-04-26T17:29:57.381433Z","shell.execute_reply.started":"2022-04-26T17:29:57.374929Z","shell.execute_reply":"2022-04-26T17:29:57.380393Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import absolute_import, division, print_function, unicode_literals\n%matplotlib inline\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.simplefilter('ignore')\nsns.set(rc={'figure.figsize' : (12, 6)})\nsns.set_style(\"darkgrid\", {'axes.grid' : True})\nimport skimage\n\n# Импортируем TensorFlow и tf.keras\nimport tensorflow as tf\nfrom tensorflow import keras","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('../input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-26T18:12:09.436172Z","iopub.execute_input":"2022-04-26T18:12:09.436506Z","iopub.status.idle":"2022-04-26T18:12:10.216532Z","shell.execute_reply.started":"2022-04-26T18:12:09.436462Z","shell.execute_reply":"2022-04-26T18:12:10.215634Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"data.shape\ndata.info()\ndata.sentiment.value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-04-26T17:30:19.358615Z","iopub.execute_input":"2022-04-26T17:30:19.359251Z","iopub.status.idle":"2022-04-26T17:30:19.398287Z","shell.execute_reply.started":"2022-04-26T17:30:19.359212Z","shell.execute_reply":"2022-04-26T17:30:19.397437Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Lets encode labels: each label is an integer value of either 0 or 1, where 0 is a negative review, and 1 is a positive review.\nfrom sklearn import preprocessing\nlabel_encoder = preprocessing.LabelEncoder()\ndata['sentiment'] = label_encoder.fit_transform(data['sentiment'])\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-26T17:30:23.787189Z","iopub.execute_input":"2022-04-26T17:30:23.787472Z","iopub.status.idle":"2022-04-26T17:30:23.940856Z","shell.execute_reply.started":"2022-04-26T17:30:23.787444Z","shell.execute_reply":"2022-04-26T17:30:23.940157Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Now, let's see the average number of words per sample\nplt.figure(figsize=(10, 6))\nplt.hist([len(sample) for sample in list(data['review'])], 50)\nplt.xlabel('Length of samples')\nplt.ylabel('Number of samples')\nplt.title('Sample length distribution')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-26T17:30:26.453845Z","iopub.execute_input":"2022-04-26T17:30:26.454768Z","iopub.status.idle":"2022-04-26T17:30:26.925497Z","shell.execute_reply.started":"2022-04-26T17:30:26.454713Z","shell.execute_reply":"2022-04-26T17:30:26.924519Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer()\n# So, we get such structure:\n#        | word1  | word2  |  word3 | word4\n# text1  |   1    |    1   |   1    |   0\n# text2  |   0    |    1   |   1    |   0\n# text3  |   2    |    1   |   0    |   0\n# text4  |   0    |    0   |   0    |   1\nvect_texts = vectorizer.fit_transform(list(data['review']))\n# ['word1', 'word2', 'word3', 'word4']\nall_ngrams = vectorizer.get_feature_names()\nnum_ngrams = min(50, len(all_ngrams))\nall_counts = vect_texts.sum(axis=0).tolist()[0]\n\nall_ngrams, all_counts = zip(*[(n, c) for c, n in sorted(zip(all_counts, all_ngrams), reverse=True)])\nngrams = all_ngrams[:num_ngrams]\ncounts = all_counts[:num_ngrams]\n\nidx = np.arange(num_ngrams)\n\n# Let's now plot a frequency distribution plot of the most seen words in the corpus.\nplt.figure(figsize=(30, 30))\nplt.bar(idx, counts, width=0.8)\nplt.xlabel('N-grams')\nplt.ylabel('Frequencies')\nplt.title('Frequency distribution of ngrams')\nplt.xticks(idx, ngrams, rotation=45)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-26T17:30:31.572268Z","iopub.execute_input":"2022-04-26T17:30:31.573235Z","iopub.status.idle":"2022-04-26T17:30:42.146819Z","shell.execute_reply.started":"2022-04-26T17:30:31.573176Z","shell.execute_reply":"2022-04-26T17:30:42.145974Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\nNGRAM_RANGE = (1, 2)\nTOP_K = 20000\nTOKEN_MODE = 'word'\nMIN_DOC_FREQ = 2\n\ndef ngram_vectorize(texts, labels):\n    kwargs = {\n        'ngram_range' : NGRAM_RANGE,\n        'dtype' : 'int32',\n        'strip_accents' : 'unicode',\n        'decode_error' : 'replace',\n        'analyzer' : TOKEN_MODE,\n        'min_df' : MIN_DOC_FREQ,\n    }\n    # Learn Vocab from train texts and vectorize train and val sets\n    tfidf_vectorizer = TfidfVectorizer(**kwargs)\n    transformed_texts = tfidf_vectorizer.fit_transform(texts)\n    \n    # Select best k features, with feature importance measured by f_classif\n    # Set k as 20000 or (if number of ngrams is less) number of ngrams   \n    selector = SelectKBest(f_classif, k=min(TOP_K, transformed_texts.shape[1]))\n    selector.fit(transformed_texts, labels)\n    transformed_texts = selector.transform(transformed_texts).astype('float32')\n    return transformed_texts\n# Vectorize the data\nvect_data = ngram_vectorize(data['review'], data['sentiment'])","metadata":{"execution":{"iopub.status.busy":"2022-04-26T17:30:50.761705Z","iopub.execute_input":"2022-04-26T17:30:50.762453Z","iopub.status.idle":"2022-04-26T17:31:25.672461Z","shell.execute_reply.started":"2022-04-26T17:30:50.762405Z","shell.execute_reply":"2022-04-26T17:31:25.671436Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"vect_data.shape\ntfidf = TfidfVectorizer()\ntr_texts = tfidf.fit_transform(data['review'])\ntr_texts.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-26T17:31:46.264300Z","iopub.execute_input":"2022-04-26T17:31:46.264648Z","iopub.status.idle":"2022-04-26T17:31:55.314284Z","shell.execute_reply.started":"2022-04-26T17:31:46.264612Z","shell.execute_reply":"2022-04-26T17:31:55.313425Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Split data to target (y) and features (X)\nX = vect_data.toarray()\ny = (np.array(data['sentiment']))\n\n# Here we split data to training and testing parts\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=13)\nprint(\"Train dataset shape: {0}, \\nTest dataset shape: {1}\".format(X_train.shape, X_test.shape))","metadata":{"execution":{"iopub.status.busy":"2022-04-26T17:31:55.317180Z","iopub.execute_input":"2022-04-26T17:31:55.317786Z","iopub.status.idle":"2022-04-26T17:32:01.471576Z","shell.execute_reply.started":"2022-04-26T17:31:55.317749Z","shell.execute_reply":"2022-04-26T17:32:01.470630Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"from tensorflow.python.keras import models\nfrom tensorflow.python.keras.layers import Dense\nfrom tensorflow.python.keras.layers import Dropout","metadata":{"execution":{"iopub.status.busy":"2022-04-26T17:32:01.472930Z","iopub.execute_input":"2022-04-26T17:32:01.473727Z","iopub.status.idle":"2022-04-26T17:32:01.478225Z","shell.execute_reply.started":"2022-04-26T17:32:01.473687Z","shell.execute_reply":"2022-04-26T17:32:01.477233Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def get_last_layer_units_and_activation(num_classes):\n    if num_classes == 2:\n        activation = 'sigmoid'\n        units = 1\n    else:\n        activation = 'softmax'\n        units = num_classes\n    return units, activation","metadata":{"execution":{"iopub.status.busy":"2022-04-26T17:32:10.158247Z","iopub.execute_input":"2022-04-26T17:32:10.159095Z","iopub.status.idle":"2022-04-26T17:32:10.164048Z","shell.execute_reply.started":"2022-04-26T17:32:10.159044Z","shell.execute_reply":"2022-04-26T17:32:10.163103Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# input shape is the vocabulary count used for the movie reviews (10,000 words)\nDROPOUT_RATE = 0.2\nUNITS = 64\nNUM_CLASSES = 2\nLAYERS = 2\ninput_shape = X_train.shape[1:]\n\nop_units, op_activation = get_last_layer_units_and_activation(NUM_CLASSES)\n\nmodel = keras.Sequential()\n# Applies Dropout to the input\nmodel.add(Dropout(rate=DROPOUT_RATE, input_shape=input_shape))\nfor _ in range(LAYERS-1):\n    model.add(Dense(units=UNITS, activation='relu'))\n    model.add(Dropout(rate=DROPOUT_RATE))\n    \nmodel.add(Dense(units=op_units, activation=op_activation))\n#model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-04-26T17:32:14.057417Z","iopub.execute_input":"2022-04-26T17:32:14.058284Z","iopub.status.idle":"2022-04-26T17:32:14.508558Z","shell.execute_reply.started":"2022-04-26T17:32:14.058235Z","shell.execute_reply":"2022-04-26T17:32:14.507298Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"LEARNING_RATE = 1e-3\n\n# Compile model with parameters\nif NUM_CLASSES == 2:\n    loss = 'binary_crossentropy'\nelse:\n    loss = 'sparse_categorical_crossentropy'\noptimizer = tf.keras.optimizers.Adam(lr=LEARNING_RATE)\nmodel.compile(optimizer=optimizer, loss=loss, metrics=['acc'])","metadata":{"execution":{"iopub.status.busy":"2022-04-26T17:32:25.966675Z","iopub.execute_input":"2022-04-26T17:32:25.967722Z","iopub.status.idle":"2022-04-26T17:32:25.991599Z","shell.execute_reply.started":"2022-04-26T17:32:25.967679Z","shell.execute_reply":"2022-04-26T17:32:25.990769Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"EPOCHS = 5\nBATCH_SIZE = 128\n\n# Create callback for early stopping on validation loss. If the loss does\n# not decrease on two consecutive tries, stop training\ncallbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2)]\n\n# Train and validate model\n# To start training, call the model.fit method—the model is \"fit\" to the training data.\n# Note that fit() will return a History object which we can use to plot training vs. validation accuracy and loss.\nhistory = model.fit(X_train, y_train, epochs=EPOCHS, validation_data=(X_test, y_test), verbose=1, batch_size=BATCH_SIZE, callbacks=callbacks)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T17:32:30.101918Z","iopub.execute_input":"2022-04-26T17:32:30.102233Z","iopub.status.idle":"2022-04-26T17:33:11.747986Z","shell.execute_reply.started":"2022-04-26T17:32:30.102204Z","shell.execute_reply":"2022-04-26T17:33:11.747113Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# Next, compare how the model performs on the test dataset:\ntest_loss, test_acc = model.evaluate(X_test, y_test, verbose=1)\nprint('Test loss:', test_loss)\nprint('Test accuracy:', test_acc)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T17:33:15.359415Z","iopub.execute_input":"2022-04-26T17:33:15.359718Z","iopub.status.idle":"2022-04-26T17:33:16.946778Z","shell.execute_reply.started":"2022-04-26T17:33:15.359689Z","shell.execute_reply":"2022-04-26T17:33:16.945717Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# Let's plot training and validation accuracy as well as loss.\ndef plot_history(history):\n    accuracy = history.history['acc']\n    val_accuracy = history.history['val_acc']\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    \n    epochs = range(1,len(accuracy) + 1)\n    \n    # Plot accuracy  \n    plt.figure(1)\n    plt.plot(epochs, accuracy, 'b', label='Training accuracy')\n    plt.plot(epochs, val_accuracy, 'g', label='Validation accuracy')\n    plt.title('Training and validation accuracy')\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    \n    # Plot loss\n    plt.figure(2)\n    plt.plot(epochs, loss, 'b', label='Training loss')\n    plt.plot(epochs, val_loss, 'g', label='Validation loss')\n    plt.title('Training and validation loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.show()\n\nplot_history(history)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T17:33:20.804659Z","iopub.execute_input":"2022-04-26T17:33:20.805744Z","iopub.status.idle":"2022-04-26T17:33:21.341611Z","shell.execute_reply.started":"2022-04-26T17:33:20.805648Z","shell.execute_reply":"2022-04-26T17:33:21.340946Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}